{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###load libraries and set parameters\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import librosa.display\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for duplicates\n",
    "First of all check if any of the selected song has the same name, and if so, to avoid problems modify the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates(walk_dir):\n",
    "    s = []\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            if \".mp3\" in filename:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                artist=root.split(\"\\\\\")[1]\n",
    "                song=filename.split(\" \",1)[1].split(\".\")[0]\n",
    "                s.append(song)\n",
    "    print(set([x for x in s if s.count(x) > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "duplicates(\"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Songs\n",
    "then we count how many songs we have per artist in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(walk_dir):\n",
    "    x = {}\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            if \".mp3\" in filename:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                artist=root.split(\"\\\\\")[1]\n",
    "                song=filename.split(\" \",1)[1].split(\".\")[0]\n",
    "                if artist not in x:\n",
    "                    x[artist] = 0\n",
    "                x[artist]+=1\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bruce Springsteen': 55, 'Coldplay': 55, 'Ed Sheeran': 55, \"Guns N' Roses\": 55, 'Michael Jackson': 55, 'Passenger': 55, 'Pink Floyd': 55, 'Queen': 55, 'Simon & Garfunkel': 55, 'The Beatles': 55}\n"
     ]
    }
   ],
   "source": [
    "count(\"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse song to extract features for visualization\n",
    "Here we provide the code to extract the mffc from the song in our database. We extract both normalized *song-by-song* and unnormalized mfcc's. These data will be used for visualization purposes and exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###set parameters:\n",
    "\n",
    "fs=6000      #sampling rate, default in librosa is 22050\n",
    "offset=30    #load song with specified offset in sec\n",
    "duration=60  #portion of the song to load in sec\n",
    "n_mfcc=12    #number of mfccs to extract, default is 20, other good value (to reduce dimension) is 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define function to extract features using parameter set above\n",
    "scaler = sklearn.preprocessing.StandardScaler()##to normalize mfcc\n",
    "def create_mfcc_visual(walk_dir):\n",
    "    x = []\n",
    "    x1 = []\n",
    "    #c = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            if \".mp3\" in filename:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                artist=root.split(\"\\\\\")[1]\n",
    "                song=filename.split(\" \",1)[1].split(\".\")[0]\n",
    "                print(\"processing:\"+song)\n",
    "                a = librosa.load(file_path,duration=duration,offset=offset,sr=fs)[0]\n",
    "                mfcc = librosa.feature.mfcc(a, sr=fs,n_mfcc=n_mfcc).T\n",
    "                mfcc_s = scaler.fit_transform(mfcc)\n",
    "                x.append({\"artist\": artist,\"song\": song,\"mfcc\":mfcc_s})\n",
    "                x1.append({\"artist\": artist,\"song\": song,\"mfcc\":mfcc})\n",
    "    return(x,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a python list of dictionaries {artist, song, mfcc} using above function\n",
    "#x1 are non-nomalized and will be useful for visualization purposes\n",
    "x,x1=create_mfcc(\"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save files with extracted features\n",
    "\n",
    "##convert mfcc from numpy array to list to be able to place them in a json file\n",
    "y=[]\n",
    "for t in x:\n",
    "    y.append({\"artist\": t[\"artist\"],\"song\": t[\"song\"],\"mfcc\":t[\"mfcc\"].tolist()})   \n",
    "y1=[]\n",
    "for t in x1:\n",
    "    y1.append({\"artist\": t[\"artist\"],\"song\": t[\"song\"],\"mfcc\":t[\"mfcc\"].tolist()})\n",
    "\n",
    "#save features\n",
    "with open('visualization-data-norm.txt', 'w') as filehandle:\n",
    "    json.dump(y, filehandle)\n",
    "with open('visualization-data-unnorm.txt', 'w') as filehandle:\n",
    "    json.dump(y1, filehandle)\n",
    "    \n",
    "#save also the sampling rate used to extract features\n",
    "with open('visualization-fs.txt', 'w') as filehandle:\n",
    "    json.dump(fs, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a variety of datasets with different parameters\n",
    "Here we create different files with features extracted with different parameters to see then how performance of our classifiers change by changing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of sampling rates\n",
    "SR=[2500,5000,7500,10000]\n",
    "#list of numbers for mfcc\n",
    "MFCC=[4,8,12,16,20]\n",
    "\n",
    "offset=30\n",
    "duration=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create mfcc with desidered parameters\n",
    "scaler = sklearn.preprocessing.StandardScaler()##to normalize mfcc\n",
    "def create_multiple_mfcc(walk_dir,SR,MFCC):\n",
    "    x={}\n",
    "    for sr in SR:\n",
    "        for n_mfcc in MFCC:\n",
    "            x[(sr,n_mfcc)]=[]\n",
    "    c = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            if \".mp3\" in filename:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                artist=root.split(\"\\\\\")[1]\n",
    "                song=filename.split(\" \",1)[1].split(\".\")[0]\n",
    "                #print(\"processing:\"+song)\n",
    "                c=c+1\n",
    "                if c%10==0:\n",
    "                    print(c)\n",
    "                for sr in SR:\n",
    "                    a = librosa.load(file_path,duration=duration,offset=offset,sr=sr)[0]\n",
    "                    for n_mfcc in MFCC:\n",
    "                        mfcc = librosa.feature.mfcc(a, sr=sr,n_mfcc=n_mfcc).T\n",
    "                        mfcc = scaler.fit_transform(mfcc)\n",
    "                        x[(sr,n_mfcc)].append({\"artist\": artist,\"song\": song,\"mfcc\":mfcc})\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n"
     ]
    }
   ],
   "source": [
    "#use above function fo create features\n",
    "x=create_multiple_mfcc(\"Dataset\",SR,MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transform x into a dictionary (with elements indexed in SR) of dictionaries (with elements indexed in MFCC)\n",
    "#of list (with elements that represent by songs) of dictionaries of the form {\"artist\", \"song\", \"mfcc\"}\n",
    "#where mfcc has been converted to list to be able to put everything in a json file\n",
    "y={}\n",
    "for sr in SR:\n",
    "    y[sr]={}\n",
    "    for n_mfcc in MFCC:\n",
    "        y[sr][n_mfcc]=[]\n",
    "            \n",
    "for key in x:\n",
    "    for t in x[key]:\n",
    "        y[key[0]][key[1]].append({\"artist\": t[\"artist\"],\"song\": t[\"song\"],\"mfcc\":t[\"mfcc\"].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save features\n",
    "with open('x.txt', 'w') as filehandle:\n",
    "    json.dump(y, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also save the used list of SR and MFCC\n",
    "with open('SR.txt', 'w') as filehandle:\n",
    "    json.dump(SR, filehandle)\n",
    "with open('MFCC.txt', 'w') as filehandle:\n",
    "    json.dump(MFCC, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
